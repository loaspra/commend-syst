{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data and Models\n",
    "In this notebook the data is loaded, preprocessed. And the baseline model is built from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text procesing\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# for the model\n",
    "import tensorflow as tf\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies: \n",
    "raw_movies = pd.read_csv('../data/raw/ml-latest-small/movies.csv', encoding='utf-8')\n",
    "# Users' ratings\n",
    "ratings_raw = pd.read_csv('../data/raw/ml-latest-small/ratings.csv', encoding='utf-8')\n",
    "# Tags: Xxx, abc\n",
    "tags_raw = pd.read_csv('../data/raw/ml-latest-small/tags.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Here I plan several hipotesis that represent new features:\n",
    "+ Encode each genre and assigna weight to it. Maybe some genres have higher ratings overall\n",
    "\n",
    "+ Assign a weight to each movie based on the number of views it has. Maybe the most viewed movies have a higher chance to receive a higher rating from a new user.\n",
    "\n",
    "+ Maybe the ratings given by the users should be weighetd based on the number of movies watched by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the name of the move (if exists)\n",
    "# dtf_products[\"name\"] = \n",
    "#   dtf_products[\"title\"].apply(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip())\n",
    "\n",
    "# For the Movie Dataframe:\n",
    "\n",
    "# copy raw dataframe:\n",
    "pro_movies = raw_movies.copy()\n",
    "\n",
    "# Extract the Year from the Title of the Movie (if its between parenthesis)\n",
    "pro_movies['Year'] = pro_movies['title'].apply(\n",
    "    lambda x: int(x.split(\"(\")[-1][:4].replace(\")\", \"\").strip()) # if there are 2 years (like 2006-2010), the first year is taken\n",
    "        if \"(\" in x else np.nan)    # if theres a ( in the Name, set the year, else, a NA\n",
    "        \n",
    "pro_movies['title'] = pro_movies['title'].apply(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip())\n",
    "\n",
    "\n",
    "# copy raw ratings data:\n",
    "pro_ratings = ratings_raw.copy()\n",
    "\n",
    "# Add date column\n",
    "pro_ratings['Date'] = pro_ratings['timestamp'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "pro_ratings.to_csv(\"../data/processed/pro_ratings.csv\", encoding='utf-8')\n",
    "\n",
    "pro_movies.to_csv(\"../data/processed/pro_movies.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of genres: \n",
    "\n",
    "# First get a list of lists (each list is the list of genres for each movie)\n",
    "aux = [i.split('|') for i in pro_movies.genres.unique()]\n",
    "# Then create a set (unique array of elements) and remove the no genres listed\n",
    "vocab = list(set(i for k in aux for i in k))\n",
    "vocab.remove('(no genres listed)')\n",
    "print(\"Genres present in the dataset: \", vocab)\n",
    "\n",
    "# Now, create a column for each genre:\n",
    "for genre in vocab:\n",
    "    pro_movies[genre] = pro_movies.genres.apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "len(set(pro_ratings.movieId) - set(pro_movies.movieId.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot encoding, in this case, is quite bad, as the dataframe shows below, theres very sparse. This means that our model will have a lot of entries that will, most of the time, receive a 0 as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_movies\n",
    "sns.heatmap(pro_movies[[i for i in vocab]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "There are several approach to take in Movie recommendations.\n",
    "+ Content based: recommend similar movies to the ones the user liked \n",
    "+ Collaborative filtering: if two users have similar ratings on some movies, one movie that one user already watched and rated it high is a good recommendation to the other user, if they didn't watched it already\n",
    "\n",
    "This two models are merged with the context of each movie (The year and the genres, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test Dataframes:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ready = pro_ratings\n",
    "\n",
    "train_df, test_df = train_test_split(ready, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of users and movies\n",
    "#users_n, movies_n = len(pro_ratings.userId.unique()), len(pro_movies.movieId.unique())\n",
    "users_n = pro_ratings.userId.max()\n",
    "movies_n = pro_ratings.movieId.max()\n",
    "\n",
    "print(users_n, movies_n)\n",
    "# define the output dimmension of the embedding layer:\n",
    "e_size = 5\n",
    "\n",
    "# define the units of the hidden layers:\n",
    "unidades = 20\n",
    "\n",
    "\n",
    "# Now define the Input layers:\n",
    "# Users:\n",
    "users_input = layers.Input(name = \"users_input\", shape = (1,))\n",
    "users_embedd = layers.Embedding(users_n + 1, e_size, name = \"Users_Embeddings\")(users_input)\n",
    "# is this necesssary ? \n",
    "users_final = layers.Reshape(name = \"users\", target_shape=(e_size , ))(users_embedd)\n",
    "\n",
    "# Movies:\n",
    "movies_input = layers.Input(name = \"movies_input\", shape = (1,))\n",
    "movies_embedd = layers.Embedding(movies_n + 1, e_size, name = \"Movies_Embeddings\")(movies_input)\n",
    "# is this necesssary ? \n",
    "movies_final = layers.Reshape(name = \"movies\", target_shape=(e_size, ))(movies_embedd)\n",
    " \n",
    "# Dot product between users and movies \n",
    "product = layers.Dot(name = 'Product', axes = 1, normalize = True)([users_final, movies_final]) \n",
    "\n",
    "# 2 hidden layers:\n",
    "# h1 = layers.Dense(name = 'H1', activation='relu', units = unidades)(product)\n",
    "# h2 = layers.Dense(name = 'H2', activation='relu', units = unidades)(h1)\n",
    "\n",
    "# Output (Rating)\n",
    "rating = layers.Dense(name = 'Rating', units = 1)(product)\n",
    "\n",
    "# __ Model Declaration ____\n",
    "model_basel = models.Model(inputs = [users_input, movies_input], outputs = rating, name = 'model_1')\n",
    "\n",
    "# Model compiling\n",
    "model_basel.compile(optimizer = 'adam', loss = 'mae', metrics = ['mean_absolute_percentage_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_basel, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training:\n",
    "EPOCHS = 15\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "hist = model_basel.fit(x=[train_df['userId'], train_df['movieId']], y = train_df['rating'], \n",
    "         validation_split = 0.3, epochs=EPOCHS, batch_size= BATCH_SIZE, shuffle = False, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_basel.predict([test_df['userId'], test_df['movieId']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = predictions\n",
    "y = test_df['rating']\n",
    "y = y.to_list()\n",
    "err_basel = []\n",
    "\n",
    "it = 0\n",
    "print(len(y_), len(y))\n",
    "for e in y_:\n",
    "    err_basel.append(abs(e[0] - y[it]))\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "Having a baseline model, the design of the final model starts here.\n",
    "\n",
    "The baselien model only takes into account Colaborative Filtering:\n",
    "+ The simmilarities between users: If several ratings from user \"X\" are the same or similar to the ones of user \"Y\" movies unwatched by \"Y\" but liked by \"X\" could be also liked by \"Y\"\n",
    "\n",
    "In contrast, the final model will take into consideration the \"Context\" of each movie. Like the genres and the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_beta = pro_ratings.copy()\n",
    "\n",
    "df_final_beta = df_final_beta.join(pro_movies.set_index('movieId'), on='movieId')\n",
    "df_final_beta.drop('genres', axis=1)\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(df_final_beta, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of users and movies\n",
    "#users_n, movies_n = len(pro_ratings.userId.unique()), len(pro_movies.movieId.unique())\n",
    "users_n = pro_ratings.userId.max()\n",
    "movies_n = pro_ratings.movieId.max()\n",
    "\n",
    "# get the number of the lookup for genres embedding layer\n",
    "genres_esize = len(vocab) + 1\n",
    "\n",
    "print(users_n, movies_n)\n",
    "# define the output dimmension of the embedding layer:\n",
    "e_size = 50\n",
    "\n",
    "# define the units of the hidden layers:\n",
    "unidades = 20\n",
    "\n",
    "\n",
    "# Now define the Input layers:\n",
    "# Users:\n",
    "users_input = layers.Input(name = \"users_input\", shape = (1,))\n",
    "users_embedd = layers.Embedding(users_n + 1, e_size, name = \"Users_Embeddings\")(users_input)\n",
    "# is this necesssary ? \n",
    "users_final = layers.Reshape(name = \"users\", target_shape=(e_size , ))(users_embedd)\n",
    "\n",
    "# Movies:\n",
    "movies_input = layers.Input(name = \"movies_input\", shape = (1,))\n",
    "movies_embedd = layers.Embedding(movies_n + 1, e_size, name = \"Movies_Embeddings\")(movies_input)\n",
    "# is this necesssary ? \n",
    "movies_final = layers.Reshape(name = \"movies\", target_shape=(e_size, ))(movies_embedd)\n",
    " \n",
    "# Dot product between users and movies \n",
    "product = layers.Dot(name = 'Product', axes = 1, normalize = True)([users_final, movies_final]) \n",
    "\n",
    "# More inputs:\n",
    "# Features (Genres)\n",
    "genres_input = layers.Input(name = 'genres', shape = (19,))\n",
    "\n",
    "# Merge both genres and dot product\n",
    "union = layers.Concatenate()([genres_input, product])\n",
    "\n",
    "# 2 hidden layers:\n",
    "h1 = layers.Dense(name = 'H1', activation='relu', units = unidades)(union)\n",
    "h2 = layers.Dense(name = 'H2', activation='relu', units = unidades)(h1)\n",
    "\n",
    "\n",
    "# Dates: (better a normalized timestamp?)\n",
    "# date = layers.Input(name = 'timestamp')\n",
    "\n",
    "# Output (Rating)\n",
    "rating = layers.Dense(name = 'Rating', units = 1)(h2)\n",
    "\n",
    "# __ Model Declaration ____\n",
    "model_final = models.Model(inputs = [users_input, movies_input, genres_input], outputs = rating, name = 'model_1')\n",
    "\n",
    "# Model compiling\n",
    "model_final.compile(optimizer = 'adam', loss = 'mae', metrics = ['mean_absolute_percentage_error'])\n",
    "\n",
    "# Plot the model\n",
    "tf.keras.utils.plot_model(model_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_df['rating']\n",
    "\n",
    "hist_final = model_final.fit(x=[train_df['userId'], train_df['movieId'], train_df[vocab]], y = train_df['rating'], \n",
    "         validation_split = 0.3, epochs=EPOCHS, batch_size= BATCH_SIZE, shuffle = False, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "predictions = model_final.predict([test_df['userId'], test_df['movieId'], test_df[vocab]])\n",
    "y_ = predictions\n",
    "y = test_df['rating']\n",
    "\n",
    "print(\"prediccions: \", predictions)\n",
    "print(\"rratings: \", test_df['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "err_end = []\n",
    "y = y.to_list()\n",
    "for pred in y_:\n",
    "    err_end.append(abs(pred[0] - y[it]))\n",
    "    it = it + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing results:\n",
    "print(\"MAE of baseline model (15 epochs): \", np.mean(err_basel))\n",
    "print(\"MAE of final model (5 epochs): \", np.mean(err_end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future proposals:\n",
    "+ Encode each genre and assign a weight to it. \n",
    "   Then predict the rating based on the weights of the genres. Maybe movies with some specific genres are more likely to receive a higher rating for the **average** user\n",
    "\n",
    "+ Assign a weight to each movie that represents how many users viewed it. \n",
    "  Maybe the most viewed movies have a better chance to be a great recommendation\n",
    "  to new users.\n",
    "\n",
    "+ The more movies a user watches the more valuable their ratings, maybe.\n",
    "\n",
    "+ Take into consideration Timestamp, maybe more recent movies are a better recommendation instead of older movies (with the same rating, like, Who watches Infinity War nowadays?)\n",
    "\n",
    "+ Also, take into account the seasons (like halloween, christmas, and so on)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('comend_NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0869c7987dcd1d9d3dbc429470df7af890c6e9c6bb454c921ba806ae0e481aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
