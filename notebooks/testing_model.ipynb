{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing data and Models\n",
    "In this notebook the data is loaded, preprocessed. And the model is built from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "\n",
    "# graphics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# text procesing\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# for the model\n",
    "import tensorflow as tf\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movies: \n",
    "raw_movies = pd.read_csv('../data/raw/ml-latest-small/movies.csv', encoding='utf-8')\n",
    "# Users' ratings\n",
    "ratings_raw = pd.read_csv('../data/raw/ml-latest-small/ratings.csv', encoding='utf-8')\n",
    "# Tags: Xxx, abc\n",
    "tags_raw = pd.read_csv('../data/raw/ml-latest-small/tags.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "Here I plan several hipotesis that represent new features:\n",
    "+ Encode each genre and assigna weight to it. Maybe some genres have higher ratings overall\n",
    "\n",
    "+ Assign a weight to each movie based on the number of views it has. Maybe the most viewed movies have a higher chance to receive a higher rating from a new user.\n",
    "\n",
    "+ Maybe the ratings given by the users should be weighetd based on the number of movies watched by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the data\n",
    "# Try to find an encode for each genre and assign a weight to it. \n",
    "#   Then predict the ratng based on the weights of the genres\n",
    "\n",
    "# Assign a weight to each movie that represents how many users viewed it. \n",
    "#   MAybe the most viewed movies have a better chance to be a great recommendation\n",
    "#   to new users\n",
    "\n",
    "# The more movies a user watchs the more valuable their ratings, maybe.\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the name of the move (if exists)\n",
    "# dtf_products[\"name\"] = \n",
    "#   dtf_products[\"title\"].apply(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip())\n",
    "\n",
    "# For the Movie Dataframe:\n",
    "\n",
    "# copy raw dataframe:\n",
    "pro_movies = raw_movies.copy()\n",
    "\n",
    "# Extract the Year from the Title of the Movie (if its between parenthesis)\n",
    "pro_movies['Year'] = pro_movies['title'].apply(\n",
    "    lambda x: int(x.split(\"(\")[-1][:4].replace(\")\", \"\").strip()) # if there are 2 years (like 2006-2010), the first year is taken\n",
    "        if \"(\" in x else np.nan)    # if theres a ( in the Name, set the year, else, a NA\n",
    "        \n",
    "pro_movies['title'] = pro_movies['title'].apply(lambda x: re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x).strip())\n",
    "\n",
    "\n",
    "# copy raw ratings data:\n",
    "pro_ratings = ratings_raw.copy()\n",
    "\n",
    "# Add date column\n",
    "pro_ratings['Date'] = pro_ratings['timestamp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of genres: \n",
    "\n",
    "# First get a list of lists (each list is the list of genres for each movie)\n",
    "aux = [i.split('|') for i in pro_movies.genres.unique()]\n",
    "# Then create a set (unique array of elements) and remove the no genres listed\n",
    "vocab = list(set(i for k in aux for i in k))\n",
    "vocab.remove('(no genres listed)')\n",
    "print(\"Genres present in the dataset: \", vocab)\n",
    "\n",
    "# Now, create a column for each genre:\n",
    "for genre in vocab:\n",
    "    pro_movies[genre] = pro_movies.genres.apply(lambda x: 1 if genre in x else 0)\n",
    "\n",
    "pro_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onehot encoding, in this case, is quite bad, as the dataframe shows below, theres very sparse. This means that our model will have a lot of entries that will, most of the time, receive a 0 as an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro_movies\n",
    "sns.heatmap(pro_movies[[i for i in vocab]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "There are several approach to take in Movie recommendations.\n",
    "+ Content based: recommend similar movies to the ones the user liked \n",
    "+ Colaborative filtering: if two users have similar ratings on some movies, one movie that one user already watched and rated it high is a good recommendation to the other user, if they didn't watched it already\n",
    "\n",
    "This two models are merged with the context of each movie (The year and the genres, in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sparse (yikes):\n",
    "tmp = pro_ratings.copy()\n",
    "dtf_users = tmp.pivot_table(index=\"userId\", columns=\"movieId\", values=\"rating\")\n",
    "missing_cols = list(set(pro_movies.index) - set(dtf_users.columns))\n",
    "for col in missing_cols:\n",
    "    dtf_users[col] = np.nan\n",
    "dtf_users = dtf_users[sorted(dtf_users.columns)]\n",
    "\n",
    "print(dtf_users.shape[0], dtf_users.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtf_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of users and movies\n",
    "users_n, movies_n = len(pro_ratings.userId.unique()), len(pro_movies.movieId.unique())\n",
    "print(users_n, movies_n)\n",
    "print(len(raw_movies.movieId.unique()))\n",
    "# define the output dimmension of the embedding layer:\n",
    "e_size = 75\n",
    "\n",
    "\n",
    "# NOW the layers:\n",
    "\n",
    "# Users:\n",
    "users_input = layers.Input(name = \"user_input\", shape = (1,))\n",
    "users_embedd = layers.Embedding(users_n, e_size)(users_input)\n",
    "# is this necesssary ? \n",
    "users_final = layers.Reshape(name = \"users\", target_shape=(e_size, ))(users_embedd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Model\n",
    "embeddings_size = 50\n",
    "usr, prd = dtf_users.shape[0], dtf_users.shape[1]\n",
    "\n",
    "# Users (1,embedding_size)\n",
    "xusers_in = layers.Input(name=\"xusers_in\", shape=(1,))\n",
    "xusers_emb = layers.Embedding(name=\"xusers_emb\", input_dim=usr, output_dim=embeddings_size)(xusers_in)\n",
    "xusers = layers.Reshape(name='xusers', target_shape=(embeddings_size,))(xusers_emb)\n",
    "\n",
    "# Products (1,embedding_size)\n",
    "xproducts_in = layers.Input(name=\"xproducts_in\", shape=(1,))\n",
    "xproducts_emb = layers.Embedding(name=\"xproducts_emb\", input_dim=prd, output_dim=embeddings_size)(xproducts_in)\n",
    "xproducts = layers.Reshape(name='xproducts', target_shape=(embeddings_size,))(xproducts_emb)\n",
    "\n",
    "# Product (1)\n",
    "xx = layers.Dot(name='xx', normalize=True, axes=1)([xusers, xproducts])\n",
    "\n",
    "# Predict ratings (1)\n",
    "y_out = layers.Dense(name=\"y_out\", units=1, activation='linear')(xx)\n",
    "\n",
    "# Compile\n",
    "model = models.Model(inputs=[xusers_in,xproducts_in], outputs=y_out, name=\"CollaborativeFiltering\")\n",
    "model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mean_absolute_percentage_error'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('comend_NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0869c7987dcd1d9d3dbc429470df7af890c6e9c6bb454c921ba806ae0e481aac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
